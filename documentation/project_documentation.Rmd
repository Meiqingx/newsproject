---
title: "Predicting Commodities Prices: A Data-Mining Approach"

author: "Rodrigo Vald√©s Ortiz, Meiqing Zhang, & Erin M. Ochoa"

date: "<p>Computer Science with Applications II: &nbsp;2017 Winter</p><p>2017 March 14</p>"
output:
  html_document:
    toc: true
    toc_depth: 2
---

<style>
  body {background-color: white;}
  h1 {color: lightseagreen;}
  h2 {color: deeppink;}
  h3 {color: blueviolet;}
  p {color: dimgray;}
  a {color: lightseagreen;}
  a:link {color: lightseagreen;}
  a:link:hover {color: white; background-color: lightseagreen;}
  a:visited {color: white; background-color: lightseagreen;}
  a:visited:hover {color: white; background-color: lightseagreen;}
}
</style>

# Description

This program forecasts the prices of ten commodities based on a pool of 192 independent predictors:  64 first-order variables and their squared and cubic terms. Running the program produces one pdf report for each of the ten commodities. 

## Technical details

Python modules are designed to run in Linux on Python 3.4+; shell scripts are designed to run on GNU bash 3.2+.  Plotting requires an update to matplotlib, which requires the installation of libffi-dev, which requires an update of apt-get.

> **How to run the program**
> 
> 1. Make sure all packages mentioned in the next section are properly installed.
>
> 1. Download the electricity dataset (http://www.eia.gov/totalenergy/data/browser/xls.cfm?tbl=T07.01) and place it in **code/parsers/electricity_file/**.
> 
> 1. CD into **code/analysis_and_plotting/**.
> 
> 1. Execute **./getdata.sh** to scrape all the data, merge and process data, produce two CSV files (one for independent variables and one for dependent variables), and $ceiling(N/50)$ diagnostic plotting graphs (where N is the number of independent variables).
> 
> 1. Execute **python3 run_files.py** to produce ten pdf reports of data analysis results; these will be saved to the **code/analysis_and_plotting/reports/** directory.
> 
> 1. Optionally, once **run_files.py** has completed, reports can be generated by executing **python3 pkl2report.py**.

## Package installation and updates

Before running the software, please execute the following commands to ensure that the necessary packages are installed and/or updated:

```{bash, eval=FALSE, highlight=TRUE, include=TRUE}
sudo pip3 install bs4
sudo pip3 install lxml
sudo pip3 install numpy
sudo pip3 install pandas
sudo pip3 install pylatex
sudo pip3 install sklearn
sudo pip3 install requests
sudo pip3 install html2text
sudo pip3 install statsmodels

sudo apt-get update
sudo apt-get install libffi-dev
sudo apt-get build-dep matplotlib
sudo pip3 install matplotlib
sudo apt-get install texlive-latex-base
sudo apt-get install texlive-latex-extra
sudo apt-get install texlive-fonts-recommended
```

Please also install dateutil from: https://pypi.python.org/pypi/python-dateutil/2.6.0

<p align=right><a href="#top" style="color:coral;background-color:white">Table of Contents</a></p>

# Data crawlers and scrapers

## Shell script to scrape and process raw data
> filename:  **code/analysis_and_plotting/get_datash.sh**

* Description: 
A shell script that runs through all data-scraping modules, the data-merging module, and the diagonistic plotting module. It produces two CSV data files, and a series of images, each containing up to 50 diagnostic plots.

## World Bank data
This module downloads data from the World Bank's GEM (Global Economic Monitor) and GEM Commodities data catalog using World Bank's API structure.  

> filename: **code/scrapers/wbdata.py**

### Reference of supported indicators  
```
> def get_countries()  

> def get_indicators()  
  
> def get_commodities()  
```

* Description: a variable from World Bank is defined as country+indicator. For example, CPI in Japan, stock index in US; a commodity is by nature an indicator, but it only supports world value: world+cotton, world+gold (world is also a country value)
```

### Downloading data

```
> def create_predictors_df(country_lst=None, indicator_lst=None, yymm=None, outfile='./worldbank/gem.csv')
```

* Description:
    By default download 40 economic indicators from the World Bank.  
40 variables = 8 countries * 5 indicators

* Parameters:  
    **country_lst**: a selected list of countries   
    **indicator_lst**: a selected list of indicators  
    **yymm**: (a tuple of integers )start and end year-month 
          Example: (198001, 201612)  
    **outfile**: an output filename   

```
> def create_commodities_df(commodity_lst=None, indicator_lst=None, yymm=None, outfile='./worldbank/gem.csv')
```

* Description:
    By default download 11 commodities by default (including crude oil, which is used as a predictor in our pool of independent variables).

* Parameters:    
    same as create_predictors_df.


## Electricity data

The Energy Information Administration provides a single file, which can be downloaded by hand:

URL = http://www.eia.gov/totalenergy/data/browser/xls.cfm?tbl=T07.01

File saved to:  code/parsers/electricity_file/MER_T07_01.csv

## Precipitation data

NASA provides 224 CSV maps of global monthly precipitation.

> filename:  **code/scrapers/precipitation-emo.py**

Main page:  http://neo.sci.gsfc.nasa.gov/view.php?datasetId=TRMM_3B43M&year=1998

The module scrapes the main page for each year in the data collection and determines the IDs of the files to be downloaded.  It then downloads the necessary files and outputs them with the year and month in the filename.

### Functions

```
create_output_dir():  Creates the output directory if it does not already exist.

create_main_url_list():  Creates a list of main-page URLs to be scraped.

get_csvs(queue):  Given a queue containing URLs of the files to be downloaded, downloads said files.

write_csv(csv_map,filename):  Given a single csv_map, outputs a csv file.

build_months(year):  Given a year, builds a list of the months for which data are available.

crawl_page(url,months_list,queue): Given a URL, the months for which data are available for that year, and the queue, updates the queue with the IDs of the monthly files.
```

## Recessions data

The National Bureau of Economic Research provides a list of the start and end dates of American economic recessions.

> filename:  **code/scrapers/recessions-emo.py**

URL = http://www.nber.org/cycles.html

The module downloads the page, parses it, and outputs a csv file of the start and end dates.

### Functions

```
create_output_dir():  Creates the output directory if it does not already exist.

get_page(url):  Given a URL, downloads that page and creates a BeautifulSoup object.

process_page(soup):  Processes a BeautifulSoup object and returns lists of the recession start and end dates.

clean_dates(dates):  Takes in a list of dates in 'Month YYYY' format and switches them to YYYY-MM format.

build_array(clean_start, clean_end):  Takes lists of cleaned start and end dates and makes an array of them.

write_csv(array):  Writes a csv file from the given array.
```

## Temperature data

NASA provides global temperature difference data.

> filename:  **code/scrapers/temperature/scraper_temp_rv.py**

Main page:  https://data.giss.nasa.gov/gistemp

The module downloads the relevant data and outputs a single CSV file.

### Functions

```
make_output_dir():  Creates directory for output data if the directory does not already exist.

get_url_list(initial_url):  Gets the list of URLs to scrape from the main page.

save_plain_text(urls_list,base_url): Saves the extracted plaintext to .txt files.

generate_df(file):  Generates a pandas dataframe from the given filename.

gen_auxdatabases_and_names(names_files):  Given a list of filenames, generates the correct names for the databases.

compatibility(df):  Given a dataframe, produces a dataframe consistent with the output of other crawler modules that are part of the software.

join_dataframes(df_list, treated_names):  Given a list of dataframes and their correct names, joins the dataframes.

print_full(x):  Given a dataframe, prints the dataframe.
```

<p align=right><a href="#top" style="color:coral;background-color:white">Table of Contents</a></p>

# Data parsing modules

## Electricity

This module parses the electricity report and outputs a properly formatted CSV file of its variables.

> filename:  **code/parsers/electricity-emo.py**

### Functions

```
create_output_dir():  Creates the output directory if it does not already exist.

date_fixe(date_int):  Fixes dates that come in int(YYYYMM) format and changes them to datetime objects.

build_df(filename): Given a filename, builds a pandas dataframe with one row per month and one column per variable.

write_csv(df):  Given a pandas dataframe, writes it to a CSV file.
```

## Precipitation

This module parses the precipitation CSV files and outputs a single CSV file.

> filename:  **code/parsers/precipitation-emo.py**

### Functions

```
gen_filenames():  Generates the filenames for the precipitation CSV maps that were downloaded via the precipitation scraper.

read_map(fname):  Creates a dataframe from the data in the file with the given filename.

calc_means(clean_map):  Calculates the global, northern, southern, and tropics precipitations means for the given map.

write_csv(array):  Given an array of monthly precipitation means, outputs a single CSV file.

process_maps(filenames):  Given a list of filenames, processes the maps found in the files and outputs a single CSV with the mean precipitation values for each month.
```

## Recessions

This module parses the recessions data and outputs a CSV file with the generated recession deltas (months elapsed since most recent recession beginning and months elapsed since most recent recession ending) for each month.

> filename:  **code/parsers/recessions-emo.py**

### Functions
```
read_file(fname):  Reads in the file and returns a cleaned dataframe.

date_fixer(datestring):  Takes in a date in 'YYYY Month' format and converts it to a datetime object.

build_df(clean_recessions):  Takes the cleaned dataframe and builds the output dataframe.

dy2dm(reldel):  Takes a relative delta obeject and converts it to delta-months.

def gen_months_list(clean_recessions):  Generates the list of YYYY-MMs ranging from the start of the first economic recession listed in the data to the present month.

gen_deltas(clean_recessions,months_list):  For each month in months_list, generates the months elapsed since the start of the most recent recession and the most recent recession ending.

get_dates(clean_recessions):  Returns the current date, the start date of the earliest recession, and the end date of the earliest recession, all as datetime objects.

write_csv(df):  Writes a dataframe to a CSV file.
```

<p align=right><a href="#top" style="color:coral;background-color:white">Table of Contents</a></p>

# Data merging module

This module unifies data from disparate sources, outputting one CSV for predictors and another for outcomes.

> filename:  **code/analysis_and_plotting/merger.py**

## Functions

```
read_files():  Reads in files as pandas dataframes.  Returns a list of dataframes with preditors and a single dataframe with outcomes.

date_fixer(datestring):  Converts dates from 'YYYY-MM' to datetime format.

merge_dfs(dataframes_list):  Recursively merges dataframes in the list together (inner join based on Date).

gen_sqrs_cbcs(df):  Generates squared and cubic terms for all variables in the given dataframe.

write_csv(df,filename):  Writes a dataframe to a CSV file.
```

<p align=right><a href="#top" style="color:coral;background-color:white">Table of Contents</a></p>

# Data analysis modules


<p align=right><a href="#top" style="color:coral;background-color:white">Table of Contents</a></p>

# Graphing modules

The software includes two graphing modules, one for diagnostic plots and one for predictive plots.

## Diagnostic plots

This module produces a series of figures.  A single figure includes diagnostic plots for all ten outcome variables, while the diagnostic plots for the predictor variables are in a series of figures, each containing up to 50 plots.

> filename: **code/analysis_and_plotting/graph_builder.py**

## Predictive plots

<p align=right><a href="#top" style="color:coral;background-color:white">Table of Contents</a></p>

# Reporting module
This module outputs a pdf report for each of the ten futures prices models. The ten reports are saved in the **./reports** folder.  
  
> filename: **code/analysis_and_plotting/reporting.py**

```
> class Report
```
Initialize an empty report in Latex with 11.5 * 8 in page size, 1 in margin by default. 


* Methods:
  
    + **add_headfoot(self, header_image)**: Take a path of a header image, and insert it in the report. Also set default footers.
    
    + **set_title(self, title, subtitle)**: Take two strings, and set report title and subtitle.  
    
    + **add_executive_summary(self, text)**: Take a text string, and insert executive summary.   
    
    + **gen_summary_text(self, results)**: Takes a dictionary of analysis results, and generate a summary text string. 
    
    + **insert_graph(self, df)**: Take a dataframe of prices prediction data, draw a plot and insert it in the report. 
  
    + **insert_table(self, results)**:  Take a dictionary of statistical results of the model, and insert a summary table in the report. 
  
    + **gen_pdf(self, filepath=None)**: Generate pdf report. Take a filepath as optional. 
    
    
```
> def build_report(df, results)
```
* Description: Take a dataframe of prediction data and a dictionary of model analysis results, and build a report. 


<p align=right><a href="#top" style="color:coral;background-color:white">Table of Contents</a></p>




```{r engine='python', eval=FALSE, highlight=TRUE, include=FALSE}
print('Hello, world')
```
