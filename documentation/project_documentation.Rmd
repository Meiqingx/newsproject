---
title: "Predicting Commodities Prices: A Data-Mining Approach"

author: "Rodrigo Vald√©s Ortiz, Meiqing Zhang, & Erin M. Ochoa"

date: "<p>Computer Science with Applications II: &nbsp;2017 Winter</p><p>2017 March 14</p>"
output:
  html_document:
    toc: true
    toc_depth: 2
---

<style>
  body {background-color: white;}
  h1 {color: lightseagreen;}
  h2 {color: deeppink;}
  h3 {color: blueviolet;}
  p {color: dimgray;}
  a {color: lightseagreen;}
  a:hover {color:white; background-color: lightseagreen;}
}
</style>

# Description

This program forecasts the prices of ten commodities based on a pool of 192 independent predictors:  64 first-order variables and their squared and cubic terms. Running the software produces one pdf report for each of the ten commodities. 

## Technical details and software execution

Python modules are designed to run in Linux on Python 3.4+; shell scripts are designed to run on GNU bash 3.2+.  Plotting requires an update to matplotlib, which requires the installation of libffi-dev, which requires an update of apt-get.

## Package installation and updates

Before running the software, please execute the following commands to ensure that the necessary packages are installed and/or updated:

```{r eval=FALSE, highlight=FALSE, include=TRUE}
sudo pip3 install bs4
sudo pip3 install lxml
sudo pip3 install numpy
sudo pip3 install pandas
sudo pip3 install pylatex
sudo pip3 install sklearn
sudo pip3 install requests
sudo pip3 install html2text
sudo pip3 install statsmodels

sudo apt-get update
sudo apt-get install libffi-dev
sudo apt-get build-dep matplotlib
sudo pip3 install matplotlib
sudo apt-get install texlive-latex-base
sudo apt-get install texlive-latex-extra
sudo apt-get install texlive-fonts-recommended
```

Please also install dateutil from: https://pypi.python.org/pypi/python-dateutil/2.6.0

## How to run the software
 
1. Make sure all packages mentioned in the previous section are properly installed.
 
1. CD into **code/analysis_and_plotting/**.
 
1. Execute **./getdata.sh** to scrape all the data, merge the data, and produce two CSV files (one for independent variables and one for dependent variables), and <i>ceiling(N/50)</i> diagnostic graphs (where N is the number of independent variables).

1. Execute **python3 run_files.py** to produce ten pdf reports of data analysis results; these will be saved to the **code/analysis_and_plotting/reports/** directory.

1. Optionally, once **run_files.py** has completed, future reports can be easily and safely generated by executing **python3 pkl2report.py**.

<p align=right><a href="#top" style="color:coral;background-color:white;">Table of Contents</a></p>

# Data crawlers and scrapers

## Shell script to scrape and process raw data
> filename:  **code/analysis_and_plotting/get_datash.sh**

This shell script runs all the data-scraping modules, the data-merging module, and the diagonistic plotting module. It produces two CSV data files and a series of images, each containing up to 50 diagnostic plots.

## World Bank data
> filename: **code/scrapers/wbdata.py**

This module downloads data from the World Bank's GEM (Global Economic Monitor) and GEM Commodities data catalog using the World Bank's API structure.  

### Reference of supported indicators
```{r eval=FALSE, highlight=FALSE, include=TRUE}
get_countries(): Returns a set of countries of interest.

get_indicators(): Returns a set of economic indicators of interest.
  
get_commodities(): Returns a set of commodities of interest.
```


### Downloading data

```{r eval=FALSE, highlight=FALSE, include=TRUE}
create_predictors_df(country_lst=None,indicator_lst=None,yymm=None,outfile='./worldbank/gem.csv'):  By default,
downloads eight economic indicators for five countries (returning 40 variables) from the World Bank.  It can
optionally take a list of countries, a list of indicators, start and end dates, and an output filename.

create_commodities_df(commodity_lst=None,indicator_lst=None,yymm=None,outfile='./worldbank/gem-commodities.csv'):
Downloads prices for 11 commodities by default.  It can optionally take a list of commodities, a list of
indicators, start and end dates, and an output filename.
```

## Electricity data
> filename: **code/parsers/electricity_file/MER_T07_01.csv**

The Energy Information Administration provides data in a single file, which is downloaded via **getdata.sh**.

Main page:  https://www.eia.gov/totalenergy/data/browser

## Precipitation data
> filename:  **code/scrapers/precipitation-emo.py**

NASA provides 224 CSV maps of global monthly precipitation.

Main page:  http://neo.sci.gsfc.nasa.gov/view.php?datasetId=TRMM_3B43M&year=1998

The module scrapes the main page for each year in the data collection and determines the IDs of the files to be downloaded.  It then downloads the necessary files and outputs them with the year and month in the filename.

### Functions

```{r eval=FALSE, highlight=FALSE, include=TRUE}
create_output_dir():  Creates the output directory if it does not already exist.

create_main_url_list():  Creates a list of main-page URLs to be scraped.

get_csvs(queue):  Given a queue containing URLs of the files to be downloaded, downloads said files.

write_csv(csv_map,filename):  Given a single csv_map, outputs a csv file.

build_months(year):  Given a year, builds a list of the months for which data are available.

crawl_page(url,months_list,queue): Given a URL, the months for which data are available for that year, and the
current queue, updates the queue with the IDs of the monthly files.
```

## Recessions data
> filename:  **code/scrapers/recessions-emo.py**

The National Bureau of Economic Research provides a list of the start and end dates of American economic recessions.

Main page: http://www.nber.org/cycles.html

The module downloads the page, parses it, and outputs a CSV file of the start and end dates.

### Functions

```{r eval=FALSE, highlight=FALSE, include=TRUE}
create_output_dir():  Creates the output directory if it does not already exist.

get_page(url):  Given a URL, downloads that page and creates a BeautifulSoup object.

process_page(soup):  Processes a BeautifulSoup object and returns lists of the recession start and end dates.

clean_dates(dates):  Takes in a list of dates in 'Month YYYY' format and switches them to YYYY-MM format.

build_array(clean_start, clean_end):  Takes lists of cleaned start and end dates and makes an array of them.

write_csv(array):  Writes a csv file from the given array.
```

## Temperature data
> filename:  **code/scrapers/temperature/scraper_temp_rv.py**

NASA provides global temperature difference data.

Main page:  https://data.giss.nasa.gov/gistemp

The module downloads the relevant data and outputs a single CSV file.

### Functions

```{r eval=FALSE, highlight=FALSE, include=TRUE}
make_output_dir():  Creates directory for output data if the directory does not already exist.

get_url_list(initial_url):  Gets the list of URLs to scrape from the main page.

save_plain_text(urls_list,base_url):  Saves the extracted plaintext to .txt files.

generate_df(file):  Generates a pandas dataframe from the given filename.

gen_auxdatabases_and_names(names_files):  Given a list of filenames, generates the correct names for the
databases.

compatibility(df):  Given a dataframe, produces a dataframe consistent with the output of other crawler modules
that are part of the software.

join_dataframes(df_list, treated_names):  Given a list of dataframes and their correct names, joins the
dataframes.

print_full(x):  Given a dataframe, prints the dataframe.
```

<p align=right><a href="#top" style="color:coral;background-color:white;">Table of Contents</a></p>

# Data-parsing modules

## Electricity
> filename:  **code/parsers/electricity-emo.py**

This module parses the electricity report and outputs a properly formatted CSV file of its variables.

### Functions

```{r eval=FALSE, highlight=FALSE, include=TRUE}
create_output_dir():  Creates the output directory if it does not already exist.

date_fixe(date_int):  Fixes dates that come in int(YYYYMM) format and changes them to datetime objects.

build_df(filename): Given a filename, builds a pandas dataframe with one row per month and one column per
variable.

write_csv(df):  Given a pandas dataframe, writes it to a CSV file.
```

## Precipitation
> filename:  **code/parsers/precipitation-emo.py**

This module parses the precipitation CSV files and outputs a single CSV file.

### Functions

```{r eval=FALSE, highlight=FALSE, include=TRUE}
gen_filenames():  Generates the filenames for the precipitation CSV maps that were downloaded via the
precipitation scraper.

read_map(fname):  Creates a dataframe from the data in the file with the given filename.

calc_means(clean_map):  Calculates the global, northern, southern, and tropics precipitations means for
the given map.

write_csv(array):  Given an array of monthly precipitation means, outputs a single CSV file.

process_maps(filenames):  Given a list of filenames, processes the maps found in the files and outputs a
single CSV with the mean precipitation values for each month.
```

## Recessions
> filename:  **code/parsers/recessions-emo.py**

This module parses the recessions data and outputs a CSV file with the generated recession deltas (months elapsed since most recent recession beginning and months elapsed since most recent recession ending) for each month.

### Functions
```{r eval=FALSE, highlight=FALSE, include=TRUE}
read_file(fname):  Reads in the file and returns a cleaned dataframe.

date_fixer(datestring):  Takes in a date in 'YYYY Month' format and converts it to a datetime object.

build_df(clean_recessions):  Takes the cleaned dataframe and builds the output dataframe.

dy2dm(reldel):  Takes a relative delta obeject and converts it to delta-months.

gen_months_list(clean_recessions):  Generates the list of YYYY-MMs ranging from the start of the first
economic recession listed in the data to the present month.

gen_deltas(clean_recessions,months_list):  For each month in months_list, generates the months elapsed
since the start of the most recent recession and the most recent recession ending.

get_dates(clean_recessions):  Returns the current date, the start date of the earliest recession, and the
end date of the earliest recession, all as datetime objects.

write_csv(df):  Writes a dataframe to a CSV file.
```

<p align=right><a href="#top" style="color:coral;background-color:white;">Table of Contents</a></p>

# Data-merging module
> filename:  **code/analysis_and_plotting/merger.py**

This module unifies data from disparate sources, outputting one CSV for predictors and another for outcomes.

## Functions

```{r eval=FALSE, highlight=FALSE, include=TRUE}
read_files():  Reads in files as pandas dataframes.  Returns a list of dataframes with preditors and a
single dataframe with outcomes.

date_fixer(datestring):  Converts dates from 'YYYY-MM' to datetime format.

merge_dfs(dataframes_list):  Recursively merges dataframes in the list together (inner join based on
Date).

gen_sqrs_cbcs(df):  Generates squared and cubic terms for all variables in the given dataframe.

write_csv(df,filename):  Writes a dataframe to a CSV file.
```

<p align=right><a href="#top" style="color:coral;background-color:white;">Table of Contents</a></p>

# Data-analysis modules

## AR simple model
> filename: **code/analysis_and_plotting/AR_model.py**

This module creates predictions for the independent variables through AR models.

### Functions

```{r eval=FALSE, highlight=FALSE, include=TRUE}
create_index(year, month, num_years_to_predict): Creates the index for the predicted part of the dataframe.

create_data_for_ar(series, num_years_to_predict): Creates series to predict the future of independent
variables (out of the sample).

AR_independent_vars(series, num_years_to_predict): Creates predictions for the independent variables.
```

## Prediction
> filename: **code/analysis_and_plotting/Predict.py**

This module creates predictions for the dependent variable.

```{r eval=FALSE, highlight=FALSE, include=TRUE}
class Predict
```

* Methods:
  
    + **model(series, autoregressive_terms, num_years_to_predict = 1, independent_vars = None)**: Creates the ARIMA model.
    
    + **predictions(model, series, num_years_to_predict = 1, independent_vars = None)**: Given an ARIMA model, predicts the values of the series.  
    
    + **residuals(model, series, independent_vars = None)**: Generates a series of residuals.   
    
    + **durbin_watson(model, series, independent_vars = None)**: Computes the Durbin - Watson statistic. 
    
    + **r_square(model, series, independent_vars = None)**: Generates the R square measure. 
  
    + **adjusted_r_square(model, series, autoregressive_terms, independent_vars = None)**: Computes the adjusted R2 . 
  
    + **mse(model, series, independent_vars = None)**: Generates the mean square error. 
    
    + **best_order(series, independent_vars = None, stop_num_lags = 1)**: Selects the number of lags. 
  
    + **best_parameters(name_column, database_dependent, database_independent, num_ind_var = 2)**: Selects the best model with X (num_ind_var) variables. 
  
    + **best_model(name_column, database_dependent, database_independent)**: Selects the best model and generates some parameters to test the model. 


## Series
> filename: **code/analysis_and_plotting/SeriesRVO.py**

This module contains functions for time-series analysis.

```{r eval=FALSE, highlight=FALSE, include=TRUE}
class Series
```

* Methods:
  
    + **create_pandas(self,file)**: Creates a dataframe with adequate characteristics for time series analysis.
    
    + **descompose(series)**: Given a series, it splits in (trend, season, residual).  
    
    + **print_seasonality(series)**: Generates a plot with trend, season, and residuals.   
    
### Functions

```{r eval=FALSE, highlight=FALSE, include=TRUE}
series_for_prediction(independent_vars, num_years_to_predict): Creates a dataframe with future values for
the independent variables.
```


<p align=right><a href="#top" style="color:coral;background-color:white;">Table of Contents</a></p>

# Visualization modules

The software includes two visualization modules, one to create diagnostic plots and one to create predictive plots.

## Diagnostic plots
> filename: **code/analysis_and_plotting/graph_builder.py**

This module produces diagnostic plots in a series of figures.  Plots for all the outcome variables are returned in a single figure.  For the predictor variables, the plots are returned in a series of figures, each containing up to 50 plots.

### Functions

```{r eval=FALSE, highlight=FALSE, include=TRUE}
plot_a_lot(df,filename):  Plots a lot of plots in one single figure.

build_plot(dates,values,label):  Generates a single plot.

plot_plots(predictors):  Breaks a dataframe into chunks of up to 50 predictor variables and generates one
figure for each chunk.
```

## Predictive plots
> filename:  **code/analysis_and_plotting/plot_pred.py**

This module produces a single plot of actual and predicted prices for a single commodity.

### Functions

```{r eval=FALSE, highlight=FALSE, include=TRUE}
build_plot(df):  Builds a plot from a dataframe of dates, actual prices, and predicted prices.
```

<p align=right><a href="#top" style="color:coral;background-color:white;">Table of Contents</a></p>

# Reporting module
> filename: **code/analysis_and_plotting/reporting.py**

This module outputs a pdf report for each of the ten futures prices models. The reports are saved in the **./reports** folder.  

```{r eval=FALSE, highlight=FALSE, include=TRUE}
class Report
```
Initialize an empty report, sized 8.5" x 11" with 1" margins. 


* Methods:
  
    + **add_headfoot(self, header_image)**: Take a path of a header image, and insert it in the report. Also set default footers.
    
    + **set_title(self, title, subtitle)**: Take two strings, and set report title and subtitle.  
    
    + **add_executive_summary(self, text)**: Take a text string, and insert executive summary.   
    
    + **gen_summary_text(self, results)**: Takes a dictionary of analysis results, and generate a summary text string. 
    
    + **insert_graph(self, df)**: Take a dataframe of prices prediction data, draw a plot, and insert it in the report. 
  
    + **insert_table(self, results)**:  Take a dictionary of statistical results of the model, and insert a summary table in the report. 
  
    + **gen_pdf(self, filepath=None)**: Generate pdf report. Take a filepath as optional. 

```{r eval=FALSE, highlight=FALSE, include=TRUE}
build_report(df, results): Takes a dataframe of prediction data and a dictionary of model statistics, and builds a report. 
```

<p align=right><a href="#top" style="color:coral;background-color:white;">Table of Contents</a></p>

# Additional scripts

## Python script to perform analyses and output reports

> filename: **code/analysis_and_plotting/run_files.py**   

This module loads databases of dependent variables and their best-fitting independent variables, and generates one report for each dependent variable in the **./reports** directory.
    
It also uses pickles to serialize the necessary data for writing the reports.  This is to provide a convenient and stable inventory for reporting materials so that future reports can be generated without undue effort.   

* Execution Process:

    * Load databases of dependent and independent variables and process them into formats that accomodate report generation.
    
    * Call the reporting module to build reports.
    
    * Serialize data with pickle at the same time.
    
    * Remove auxiliary files (such as .aux, .log, .tex) from the output directory.

### Auxiliary functions
```{r eval=FALSE, highlight=FALSE, include=TRUE}
create_pickles_dir():  Creates a directory in which to store serialized pickle data.  
    
load_data(dependent_f, independent_f):  Loads the necessary data in two databases: dependent and
independent.

create_one_output(name_var, dependent, independent, num_years_to_predict = 1):  Processes a single
predictive model and returns the data for that model's independent and dependent variables, as well as the
model statistics.

generate_outputs(dependent, independent, num_years_to_predict = 1):  Returns model data and statistics for
all predictive models.
```

## Python script to generate reports from serialized data  

> filename: **pkl2report.py**   

This module generates reports from pickles, which each contains a (dataframe, dictionary) tuple for a given commodity's model. It stands as a demonstration of how reports can be generated from stored data without massive module importing and data processing.    

<p align=right><a href="#top" style="color:coral;background-color:white;">Table of Contents</a></p><br>

