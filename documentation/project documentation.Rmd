---
title: "Predicting Commodities Prices: A Data-Mining Approach"
author: "Rodrigo Valdés Ortiz, Meiqing Zhang, & Erin M. Ochoa"

date: "2017 March 14"
output:
  html_document:
    toc: true
---

# Description
192 independent variables

Python modules are designed to run on Python 3.4+; shell scripts are designed to run on GNU bash 3.2+.

# Package installation before using the software
```{}
pip3 install bs4
pip3 install lxml
pip3 install numpy
pip3 install urllib
pip3 install pandas
pip3 install pylatex
pip3 install sklearn
pip3 install requests
pip3 install dateutil
pip3 install html2text
pip3 install matplotlib
pip3 install statsmodels

sudo apt-get install texlive-latex-base
sudo apt-get install texlive-latex-extra
sudo apt-get install texlive-fonts-recommended
```

# Data crawlers and scrapers

## World Bank data
This module downloads data from the World Bank API.  
   
—filename: wbdata.py    

### Reference of supported indicators  
```
def get_countries()  
  
def get_indicators()  
  
def get_commodities()  

```

### Downloading data

```
def create_predictors_df(country_lst=None, indicator_lst=None, yymm=None, outfile='./worldbank/gem.csv')

Download 40 economic indicators from the world bank  
Parameters:  



def create_commodities_df(commodity_lst=None, indicator_lst=None, yymm=None, outfile='./worldbank/gem.csv')

Download 11 commodities by default  
Parameters:  

```

## Electricity Data
The Energy Information Administration provides a single file, which can be downloaded by hand:

URL = http://www.eia.gov/totalenergy/data/browser/xls.cfm?tbl=T07.01

## Precipitation Data
NASA provides 224 CSV maps of global monthly precipitation.

Main page:  http://neo.sci.gsfc.nasa.gov/view.php?datasetId=TRMM_3B43M&year=1998

The module scrapes the main page for each year in the data collection and determines the IDs of the files to be downloaded.  It then downloads the necessary files and outputs them with the year and month in the filename.

### Functions
```
create_output_dir():  Creates the output directory if it does not already exist.

create_main_url_list():  Creates a list of main-page URLs to be scraped.

get_csvs(queue):  Given a queue containing URLs of the files to be downloaded, downloads said files.

write_csv(csv_map,filename):  Given a single csv_map, outputs a csv file.

build_months(year):  Given a year, builds a list of the months for which data are available.

crawl_page(url,months_list,queue): Given a URL, the months for which data are available for that year, and the queue, updates the queue with the IDs of the monthly files.
```
## Recessions Data

The National Bureau of Economic Research provides a list of the start and end dates of American economic recessions.

URL = http://www.nber.org/cycles.html

The module downloads the page, parses it, and outputs a csv file of the start and end dates.

### Functions
```
create_output_dir():  Creates the output directory if it does not already exist.

get_page(url):  Given a URL, downloads that page and creates a BeautifulSoup object.

process_page(soup):  Processes a BeautifulSoup object and returns lists of the recession start and end dates.

clean_dates(dates):  Takes in a list of dates in 'Month YYYY' format and switches them to YYYY-MM format.

build_array(clean_start, clean_end):  Takes lists of cleaned start and end dates and makes an array of them.

write_csv(array):  Writes a csv file from the given array.
```


## Temperature data
NASA provides global temperature difference data.

Main page:  https://data.giss.nasa.gov/gistemp

The module downloads the relevant data and outputs a single CSV file.

### Functions
```
make_output_dir():  Creates directory for output data if the directory does not already exist.

get_url_list(initial_url):  Gets the list of URLs to scrape from the main page.

save_plain_text(urls_list,base_url): Saves the extracted plaintext to .txt files.

generate_df(file):  Generates a pandas dataframe from the given filename.

gen_auxdatabases_and_names(names_files):  Given a list of filenames, generates the correct names for the databases.

compatibility(df):  Given a dataframe, produces a dataframe consistent with the output of other crawler modules that are part of the software.

join_dataframes(df_list, treated_names):  Given a list of dataframes and their correct names, joins the dataframes.

print_full(x):  Given a dataframe, prints the dataframe.
```

# Data parsing modules

## Electricity

This module parses the electricity report and outputs a properly formatted CSV file of its variables.

### Functions
```
create_output_dir():  Creates the output directory if it does not already exist.

date_fixe(date_int):  Fixes dates that come in int(YYYYMM) format and changes them to datetime objects.

build_df(filename): Given a filename, builds a pandas dataframe with one row per month and one column per variable.

write_csv(df):  Given a pandas dataframe, writes it to a CSV file.
```

## Precipitation

## Recessions

# Data analysis modules



# Graphing module



# Reporting module
This module creates report  
  
filename: reporting.py  

```
class Report

Parameters:

Methods:
  set_title()

  insert_graph()

  insert_table()

  generate_pdf()
```



```{r engine='python', eval=FALSE, highlight=TRUE, include=TRUE}
print('Hello, world')
```
